{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6bd819-4839-4f84-a872-9b6a4c753a53",
   "metadata": {},
   "source": [
    "# Business problem:\n",
    "**Classify the images & predict on future on data(whether the image belongs to which class) with maximum accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f28824-c5aa-4eb5-90af-e676c176b6ff",
   "metadata": {},
   "source": [
    "# Data understanding:\n",
    "\n",
    "- given multiple images of cats & dogs.\n",
    "- Having different folders for each (cats & dogs)\n",
    "- 2 classes --> Binaru Classification Project\n",
    "- given images, of different size(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379f34ee-cd0a-42a1-90b9-5a82d58201fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191aaaa6-ce14-499d-8188-05affb00f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218b2aa-98e4-45e3-9646-7278f85267b8",
   "metadata": {},
   "source": [
    "# preprocessing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55745f5b-b55b-42ad-95c6-254100c2d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen= ImageDataGenerator( rescale = 1/255,zoom_range = 0.2,shear_range = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f30b8cf8-640a-463f-8c63-afbcf0b3772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8048 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = gen.flow_from_directory('E:\\\\New MachineLearning\\\\7.CNN\\dataset\\\\training_set',\n",
    "                                       target_size = (64,64), class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f18469b1-e762-4ff2-bf1a-391518d7fedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c330604d-b73d-4bf8-990b-d72a2b57a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test= ImageDataGenerator( rescale = 1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4afaf8ad-d39a-4968-90ce-6b3382ae2238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test.flow_from_directory('E:\\\\New MachineLearning\\\\7.CNN\\dataset\\\\test_set',\n",
    "                                       target_size = (64,64), class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d1363-99eb-40fb-8d33-ed73ba74d77b",
   "metadata": {},
   "source": [
    "# Modeling - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "121a07da-914b-40ff-a41e-5eea8649a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization CNN\n",
    "\n",
    "from keras.models import Sequential\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a272a54e-d329-4b2a-8dd0-92160ec11b8c",
   "metadata": {},
   "source": [
    "# Step1 : Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3f4c22a-43d9-49f0-a922-b10ee079db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "\n",
    "classifier.add(Conv2D(input_shape = [64,64,3],\n",
    "                      filters = 32,\n",
    "                      kernel_size = 3,\n",
    "                     activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66bd298-d759-43f5-b855-a6fdaa493667",
   "metadata": {},
   "source": [
    "# Step2 : Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09477fb8-4801-4865-8988-8d92e23895ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab1421-91e1-4b22-b187-9b1136729d5a",
   "metadata": {},
   "source": [
    "# Step3 : Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2816eb74-7d85-4381-96e6-0f4454ba8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe42a8f-ef53-4ec0-8327-710be4559604",
   "metadata": {},
   "source": [
    "# Step4 : Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2384ab80-6f27-4140-9572-8e76096d7dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# hidden layer \n",
    "classifier.add(Dense(units = 128,activation = 'relu'))\n",
    "\n",
    "# output layer\n",
    "classifier.add(Dense(units=1 , activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ea312e-18b1-4516-bc17-e2b85cab3863",
   "metadata": {},
   "source": [
    "**Compile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c26f9517-20ec-45db-9610-e5dfd714134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam',\n",
    "                   loss = 'binary_crossentropy',\n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ed139-57d8-4ff9-bd33-b121eacde224",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6985d4e5-2e64-445b-828e-f4c527b06af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 993ms/step - accuracy: 0.5010 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 2/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 457ms/step - accuracy: 0.4970 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 3/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 199ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 4/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 195ms/step - accuracy: 0.4985 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 5/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 194ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 6/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 197ms/step - accuracy: 0.4990 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 7/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 424ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 8/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 559ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 9/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 199ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 10/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 200ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 11/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 198ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 12/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 200ms/step - accuracy: 0.4995 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 13/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 211ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 14/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 200ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 15/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 197ms/step - accuracy: 0.5005 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 16/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 198ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 17/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 203ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 18/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 200ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 19/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 199ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 20/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 200ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 21/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 210ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 22/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 201ms/step - accuracy: 0.4990 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 23/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 202ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 24/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 198ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 25/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 200ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22b4790e420>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x=training_set,validation_data = test_set, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cce87f-8e62-4593-994f-3ddc2744bb16",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "**Making single prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d632c2d-ae8b-41e7-9ed0-71bff2a998d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced74525-7af0-4780-8d38-8b777926dd5f",
   "metadata": {},
   "source": [
    "**load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "79459040-2ffe-4142-9c68-71300ac8309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = Image.open('E:\\\\New MachineLearning\\\\7.CNN\\\\dataset\\\\single_prediction\\\\cat.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e157900f-713a-43a5-8425-832ad5d05b1a",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "91abab6c-744f-41ec-91cd-4a49b600ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = test_image.resize((64,64))\n",
    "test_image = np.array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8b2e5-6377-4dd8-bdc6-e76836d9103d",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "95017136-4ae0-4f7f-8ddc-1535639a3ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a4199-2fee-41c0-a95e-b8e5656a67ed",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fa5b28f3-c6a9-4a4c-b70e-f1810c96bb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat\n"
     ]
    }
   ],
   "source": [
    "if result[0][0]==1:\n",
    "    print(\"Dog\")\n",
    "else:\n",
    "    print(\"Cat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
