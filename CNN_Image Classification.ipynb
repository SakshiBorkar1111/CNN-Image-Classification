{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6bd819-4839-4f84-a872-9b6a4c753a53",
   "metadata": {},
   "source": [
    "# Business problem:\n",
    "**Classify the images & predict on future on data(whether the image belongs to which class) with maximum accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f28824-c5aa-4eb5-90af-e676c176b6ff",
   "metadata": {},
   "source": [
    "# Data understanding:\n",
    "\n",
    "- given multiple images of cats & dogs.\n",
    "- Having different folders for each (cats & dogs)\n",
    "- 2 classes --> Binaru Classification Project\n",
    "- given images, of different size(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379f34ee-cd0a-42a1-90b9-5a82d58201fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "191aaaa6-ce14-499d-8188-05affb00f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218b2aa-98e4-45e3-9646-7278f85267b8",
   "metadata": {},
   "source": [
    "# preprocessing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55745f5b-b55b-42ad-95c6-254100c2d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen= ImageDataGenerator( rescale = 1/255,zoom_range = 0.2,shear_range = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f30b8cf8-640a-463f-8c63-afbcf0b3772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8048 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = gen.flow_from_directory('E:\\\\AI-ML_SB\\\\7.CNN\\\\dataset\\\\training_set',\n",
    "                                       target_size = (64,64), class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f18469b1-e762-4ff2-bf1a-391518d7fedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c330604d-b73d-4bf8-990b-d72a2b57a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test= ImageDataGenerator( rescale = 1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4afaf8ad-d39a-4968-90ce-6b3382ae2238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test.flow_from_directory('E:\\\\AI-ML_SB\\\\7.CNN\\\\dataset\\\\test_set',\n",
    "                                       target_size = (64,64), class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d1363-99eb-40fb-8d33-ed73ba74d77b",
   "metadata": {},
   "source": [
    "# Modeling - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121a07da-914b-40ff-a41e-5eea8649a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization CNN\n",
    "\n",
    "from keras.models import Sequential\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a272a54e-d329-4b2a-8dd0-92160ec11b8c",
   "metadata": {},
   "source": [
    "# Step1 : Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3f4c22a-43d9-49f0-a922-b10ee079db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "\n",
    "classifier.add(Conv2D(input_shape = [64,64,3],\n",
    "                      filters = 32,\n",
    "                      kernel_size = 3,\n",
    "                     activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66bd298-d759-43f5-b855-a6fdaa493667",
   "metadata": {},
   "source": [
    "# Step2 : Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09477fb8-4801-4865-8988-8d92e23895ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab1421-91e1-4b22-b187-9b1136729d5a",
   "metadata": {},
   "source": [
    "# Step3 : Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2816eb74-7d85-4381-96e6-0f4454ba8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe42a8f-ef53-4ec0-8327-710be4559604",
   "metadata": {},
   "source": [
    "# Step4 : Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2384ab80-6f27-4140-9572-8e76096d7dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# hidden layer \n",
    "classifier.add(Dense(units = 128,activation = 'relu'))\n",
    "\n",
    "# output layer\n",
    "classifier.add(Dense(units=1 , activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ea312e-18b1-4516-bc17-e2b85cab3863",
   "metadata": {},
   "source": [
    "**Compile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c26f9517-20ec-45db-9610-e5dfd714134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam',\n",
    "                   loss = 'binary_crossentropy',\n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ed139-57d8-4ff9-bd33-b121eacde224",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6985d4e5-2e64-445b-828e-f4c527b06af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 1s/step - accuracy: 0.6252 - loss: 0.6609 - val_accuracy: 0.6865 - val_loss: 0.5874\n",
      "Epoch 2/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 597ms/step - accuracy: 0.6914 - loss: 0.5842 - val_accuracy: 0.7110 - val_loss: 0.5636\n",
      "Epoch 3/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 335ms/step - accuracy: 0.7116 - loss: 0.5617 - val_accuracy: 0.7185 - val_loss: 0.5602\n",
      "Epoch 4/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 151ms/step - accuracy: 0.7353 - loss: 0.5353 - val_accuracy: 0.7090 - val_loss: 0.5795\n",
      "Epoch 5/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 155ms/step - accuracy: 0.7408 - loss: 0.5145 - val_accuracy: 0.7400 - val_loss: 0.5348\n",
      "Epoch 6/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 155ms/step - accuracy: 0.7561 - loss: 0.4997 - val_accuracy: 0.7400 - val_loss: 0.5307\n",
      "Epoch 7/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 150ms/step - accuracy: 0.7637 - loss: 0.4861 - val_accuracy: 0.7355 - val_loss: 0.5531\n",
      "Epoch 8/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 155ms/step - accuracy: 0.7739 - loss: 0.4679 - val_accuracy: 0.7090 - val_loss: 0.5770\n",
      "Epoch 9/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 155ms/step - accuracy: 0.7863 - loss: 0.4549 - val_accuracy: 0.7245 - val_loss: 0.6388\n",
      "Epoch 10/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 156ms/step - accuracy: 0.7934 - loss: 0.4357 - val_accuracy: 0.7620 - val_loss: 0.5339\n",
      "Epoch 11/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 155ms/step - accuracy: 0.8012 - loss: 0.4248 - val_accuracy: 0.7280 - val_loss: 0.6165\n",
      "Epoch 12/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 172ms/step - accuracy: 0.8084 - loss: 0.4109 - val_accuracy: 0.7580 - val_loss: 0.5590\n",
      "Epoch 13/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 150ms/step - accuracy: 0.8334 - loss: 0.3800 - val_accuracy: 0.7575 - val_loss: 0.5725\n",
      "Epoch 14/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.8330 - loss: 0.3736 - val_accuracy: 0.7515 - val_loss: 0.6441\n",
      "Epoch 15/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 157ms/step - accuracy: 0.8423 - loss: 0.3507 - val_accuracy: 0.7585 - val_loss: 0.5907\n",
      "Epoch 16/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 153ms/step - accuracy: 0.8557 - loss: 0.3279 - val_accuracy: 0.7570 - val_loss: 0.5777\n",
      "Epoch 17/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - accuracy: 0.8705 - loss: 0.3042 - val_accuracy: 0.7565 - val_loss: 0.6276\n",
      "Epoch 18/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 152ms/step - accuracy: 0.8749 - loss: 0.2954 - val_accuracy: 0.7540 - val_loss: 0.6153\n",
      "Epoch 19/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 155ms/step - accuracy: 0.8836 - loss: 0.2815 - val_accuracy: 0.7565 - val_loss: 0.6688\n",
      "Epoch 20/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 142ms/step - accuracy: 0.8902 - loss: 0.2557 - val_accuracy: 0.7530 - val_loss: 0.7277\n",
      "Epoch 21/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 149ms/step - accuracy: 0.8955 - loss: 0.2482 - val_accuracy: 0.7555 - val_loss: 0.6910\n",
      "Epoch 22/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 151ms/step - accuracy: 0.9139 - loss: 0.2202 - val_accuracy: 0.7655 - val_loss: 0.7299\n",
      "Epoch 23/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 143ms/step - accuracy: 0.9171 - loss: 0.2060 - val_accuracy: 0.7335 - val_loss: 0.8390\n",
      "Epoch 24/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 143ms/step - accuracy: 0.9242 - loss: 0.1912 - val_accuracy: 0.7525 - val_loss: 0.7674\n",
      "Epoch 25/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 144ms/step - accuracy: 0.9318 - loss: 0.1790 - val_accuracy: 0.7575 - val_loss: 0.7898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1dbdd7a3bd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x=training_set,validation_data = test_set, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cce87f-8e62-4593-994f-3ddc2744bb16",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "**Making single prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d632c2d-ae8b-41e7-9ed0-71bff2a998d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced74525-7af0-4780-8d38-8b777926dd5f",
   "metadata": {},
   "source": [
    "**load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79459040-2ffe-4142-9c68-71300ac8309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = Image.open('E:\\\\AI-ML_SB\\\\7.CNN\\\\dataset\\\\single_prediction\\\\cat.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e157900f-713a-43a5-8425-832ad5d05b1a",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91abab6c-744f-41ec-91cd-4a49b600ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = test_image.resize((64,64))\n",
    "test_image = np.array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8b2e5-6377-4dd8-bdc6-e76836d9103d",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95017136-4ae0-4f7f-8ddc-1535639a3ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a4199-2fee-41c0-a95e-b8e5656a67ed",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa5b28f3-c6a9-4a4c-b70e-f1810c96bb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog\n"
     ]
    }
   ],
   "source": [
    "if result[0][0]==1:\n",
    "    print(\"Dog\")\n",
    "else:\n",
    "    print(\"Cat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env311]",
   "language": "python",
   "name": "conda-env-tf_env311-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
